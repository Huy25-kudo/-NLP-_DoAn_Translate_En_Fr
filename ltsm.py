import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import os
import sys
import time
import random
import spacy
import nltk
from nltk.translate.bleu_score import corpus_bleu

# ==============================================================================
# PH·∫¶N 1: DATA UTILS (X·ª≠ l√Ω d·ªØ li·ªáu, Tokenizer, Vocabulary, Dataset)
# ==============================================================================

# --- T·∫£i Model Ng√¥n Ng·ªØ Spacy ---
try:
    spacy.load('en_core_web_sm')
    spacy.load('fr_core_news_sm')
except OSError:
    print("üî∏ ƒêang t·∫£i g√≥i ng√¥n ng·ªØ Spacy...")
    os.system("python -m spacy download en_core_web_sm")
    os.system("python -m spacy download fr_core_news_sm")

def get_tokenizer(tokenizer_type, language='en_core_web_sm'):
    if tokenizer_type == 'spacy':
        spacy_model = spacy.load(language)
        return lambda text: [tok.text for tok in spacy_model.tokenizer(text)]
    else:
        raise ValueError("Code n√†y ch·ªâ h·ªó tr·ª£ tokenizer='spacy'")

# Kh·ªüi t·∫°o tokenizer
en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')
fr_tokenizer = get_tokenizer('spacy', language='fr_core_news_sm')

def tokenize_en(text):
    return en_tokenizer(text)

def tokenize_fr(text):
    return fr_tokenizer(text)

# --- Class Vocabulary ---
class Vocabulary:
    def __init__(self, freq_threshold=2):
        self.itos = {0: "<pad>", 1: "<sos>", 2: "<eos>", 3: "<unk>"}
        self.stoi = {"<pad>": 0, "<sos>": 1, "<eos>": 2, "<unk>": 3}
        self.freq_threshold = freq_threshold

    def __len__(self):
        return len(self.itos)

    def build_vocabulary(self, sentence_list):
        frequencies = Counter()
        idx = 4
        for sentence in sentence_list:
            for word in sentence:
                frequencies[word] += 1
                if frequencies[word] == self.freq_threshold:
                    self.stoi[word] = idx
                    self.itos[idx] = word
                    idx += 1

    def numericalize(self, text):
        return [
            self.stoi[token] if token in self.stoi else self.stoi["<unk>"]
            for token in text
        ]

# --- Class Dataset ---
class Multi30kDataset(Dataset):
    def __init__(self, src_file, trg_file, src_vocab=None, trg_vocab=None):
        if not os.path.exists(src_file):
            raise FileNotFoundError(f"‚ùå KH√îNG T√åM TH·∫§Y FILE: {src_file}")
        
        print(f"üîπ ƒêang ƒë·ªçc: {os.path.basename(src_file)}...")
        self.src_data = [line.strip() for line in open(src_file, 'r', encoding='utf-8')]
        self.trg_data = [line.strip() for line in open(trg_file, 'r', encoding='utf-8')]
        
        self.src_tokenized = [tokenize_en(text) for text in self.src_data]
        self.trg_tokenized = [tokenize_fr(text) for text in self.trg_data]

        if src_vocab is None:
            self.src_vocab = Vocabulary()
            self.src_vocab.build_vocabulary(self.src_tokenized)
        else:
            self.src_vocab = src_vocab

        if trg_vocab is None:
            self.trg_vocab = Vocabulary()
            self.trg_vocab.build_vocabulary(self.trg_tokenized)
        else:
            self.trg_vocab = trg_vocab

    def __len__(self):
        return len(self.src_data)

    def __getitem__(self, index):
        src_text = self.src_tokenized[index]
        trg_text = self.trg_tokenized[index]

        src_num = [self.src_vocab.stoi["<sos>"]] + self.src_vocab.numericalize(src_text) + [self.src_vocab.stoi["<eos>"]]
        trg_num = [self.trg_vocab.stoi["<sos>"]] + self.trg_vocab.numericalize(trg_text) + [self.trg_vocab.stoi["<eos>"]]

        return torch.tensor(src_num), torch.tensor(trg_num)

# --- Class Collate ---
class Collate:
    def __init__(self, pad_idx):
        self.pad_idx = pad_idx

    def __call__(self, batch):
        batch.sort(key=lambda x: len(x[0]), reverse=True)
        
        src = [item[0] for item in batch]
        trg = [item[1] for item in batch]
        
        src_lens = torch.tensor([len(x) for x in src])
        
        src = torch.nn.utils.rnn.pad_sequence(src, batch_first=False, padding_value=self.pad_idx)
        trg = torch.nn.utils.rnn.pad_sequence(trg, batch_first=False, padding_value=self.pad_idx)
        
        return src, trg, src_lens

# ==============================================================================
# PH·∫¶N 2: MODEL (Encoder, Decoder, Seq2Seq)
# ==============================================================================

class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim)
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_len):
        embedded = self.dropout(self.embedding(src))
        # Pack sequence ƒë·ªÉ x·ª≠ l√Ω ƒë·ªô d√†i thay ƒë·ªïi
        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, src_len.to('cpu'), enforce_sorted=True)
        packed_outputs, (hidden, cell) = self.rnn(packed_embedded)
        return hidden, cell

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):
        super().__init__()
        self.output_dim = output_dim
        self.embedding = nn.Embedding(output_dim, emb_dim)
        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)
        self.fc_out = nn.Linear(hid_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, input, hidden, cell):
        input = input.unsqueeze(0)
        embedded = self.dropout(self.embedding(input))
        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))
        prediction = self.fc_out(output.squeeze(0))
        return prediction, hidden, cell

class Seq2Seq(nn.Module):
    def __init__(self, encoder, decoder, device):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.device = device

    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):
        batch_size = src.shape[1]
        trg_len = trg.shape[0]
        trg_vocab_size = self.decoder.output_dim
        
        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)
        
        hidden, cell = self.encoder(src, src_len)
        input = trg[0, :]
        
        for t in range(1, trg_len):
            output, hidden, cell = self.decoder(input, hidden, cell)
            outputs[t] = output
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1) 
            input = trg[t] if teacher_force else top1
            
        return outputs

# ==============================================================================
# PH·∫¶N 3: MAIN (Train, Eval, Inference Loop)
# ==============================================================================

def init_weights(m):
    for name, param in m.named_parameters():
        nn.init.uniform_(param.data, -0.08, 0.08)

def train(model, iterator, optimizer, criterion, clip, scaler, device):
    model.train()
    epoch_loss = 0
    for i, (src, trg, src_len) in enumerate(iterator):
        src, trg = src.to(device), trg.to(device)
        optimizer.zero_grad()
        
        with torch.amp.autocast('cuda'):
            output = model(src, src_len, trg)
            output_dim = output.shape[-1]
            output = output[1:].view(-1, output_dim)
            trg = trg[1:].view(-1)
            loss = criterion(output, trg)
        
        scaler.scale(loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        scaler.step(optimizer)
        scaler.update()
        
        epoch_loss += loss.item()
        if i % 50 == 0:
            print(f"   Batch {i}/{len(iterator)} | Loss: {loss.item():.4f}")
    return epoch_loss / len(iterator)

# H√†m h·ªó tr·ª£ t√≠nh BLEU (tr·∫£ v·ªÅ list tokens)
def translate_sentence_internal(sentence, src_vocab, trg_vocab, model, device, max_len=50):
    model.eval()
    if isinstance(sentence, str):
        tokens = tokenize_en(sentence)
    else:
        tokens = sentence
    tokens = ["<sos>"] + tokens + ["<eos>"]
    src_indexes = src_vocab.numericalize(tokens)
    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)
    src_len = torch.LongTensor([len(src_indexes)])
    
    with torch.no_grad():
        hidden, cell = model.encoder(src_tensor, src_len)
    
    trg_indexes = [trg_vocab.stoi["<sos>"]]
    for i in range(max_len):
        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)
        with torch.no_grad():
            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)
        pred_token = output.argmax(1).item()
        trg_indexes.append(pred_token)
        if pred_token == trg_vocab.stoi["<eos>"]:
            break
            
    trg_tokens = [trg_vocab.itos[i] for i in trg_indexes]
    return trg_tokens[1:-1] # B·ªè sos v√† eos

def calculate_bleu(data, src_vocab, trg_vocab, model, device):
    targets = []
    outputs = []
    print("   ƒêang t√≠nh BLEU...")
    # L·∫•y m·∫´u t·ªëi ƒëa 200 c√¢u ƒë·ªÉ test cho nhanh
    for i in range(min(len(data), 200)):
        src = data.src_tokenized[i]
        trg = data.trg_tokenized[i]
        pred = translate_sentence_internal(src, src_vocab, trg_vocab, model, device)
        targets.append([trg])
        outputs.append(pred)
    return corpus_bleu(targets, outputs)

if __name__ == "__main__":
    # --- 1. Thi·∫øt l·∫≠p ---
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"üîπ ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")
    
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(current_dir, 'data', 'multi30k_en_fr')
    
    # ƒê∆∞·ªùng d·∫´n file d·ªØ li·ªáu
    train_src = os.path.join(data_dir, 'train.en')
    train_trg = os.path.join(data_dir, 'train.fr')
    val_src = os.path.join(data_dir, 'val.en') 
    val_trg = os.path.join(data_dir, 'val.fr')

    # --- 2. Kh·ªüi t·∫°o D·ªØ li·ªáu ---
    print("\n1. KH·ªûI T·∫†O D·ªÆ LI·ªÜU")
    train_dataset = Multi30kDataset(train_src, train_trg)
    
    # N·∫øu c√≥ file validation th√¨ d√πng, kh√¥ng th√¨ d√πng t·∫°m train_dataset ƒë·ªÉ test code
    if os.path.exists(val_src):
        val_dataset = Multi30kDataset(val_src, val_trg, src_vocab=train_dataset.src_vocab, trg_vocab=train_dataset.trg_vocab)
    else:
        val_dataset = train_dataset

    BATCH_SIZE = 128
    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, 
                              collate_fn=Collate(train_dataset.src_vocab.stoi["<pad>"]),
                              num_workers=0, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, 
                             collate_fn=Collate(train_dataset.src_vocab.stoi["<pad>"]),
                             num_workers=0, pin_memory=True)

    # --- 3. Kh·ªüi t·∫°o Model ---
    print(f"\n2. KH·ªûI T·∫†O MODEL")
    INPUT_DIM = len(train_dataset.src_vocab)
    OUTPUT_DIM = len(train_dataset.trg_vocab)
    ENC_EMB_DIM = 256
    DEC_EMB_DIM = 256
    HID_DIM = 512
    N_LAYERS = 2
    ENC_DROPOUT = 0.5
    DEC_DROPOUT = 0.5
    EPOCHS = 16
    
    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)
    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)
    model = Seq2Seq(enc, dec, device).to(device)
    model.apply(init_weights)
    
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.trg_vocab.stoi["<pad>"])
    scaler = torch.amp.GradScaler('cuda')

    # --- 4. Hu·∫•n luy·ªán ---
    print(f"\n3. B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN")
    best_valid_loss = float('inf')
    patience = 3
    no_improve_epoch = 0
    model_save_path = os.path.join(current_dir, 'best-model.pth')
    
    # Ki·ªÉm tra n·∫øu ƒë√£ c√≥ model th√¨ c√≥ th·ªÉ load (optional), ·ªü ƒë√¢y ta train l·∫°i t·ª´ ƒë·∫ßu ho·∫∑c train ti·∫øp
    # N·∫øu mu·ªën ch·ªâ Inference th√¨ comment v√≤ng for loop n√†y l·∫°i v√† load model.
    
    for epoch in range(EPOCHS):
        start_time = time.time()
        
        train_loss = train(model, train_loader, optimizer, criterion, 1, scaler, device)
        
        model.eval()
        valid_loss = 0
        with torch.no_grad():
            for src, trg, src_len in val_loader:
                src, trg = src.to(device), trg.to(device)
                output = model(src, src_len, trg, 0) # 0 = turn off teacher forcing
                output = output[1:].view(-1, output.shape[-1])
                trg = trg[1:].view(-1)
                loss = criterion(output, trg)
                valid_loss += loss.item()
        valid_loss /= len(val_loader)
        
        mins, secs = divmod(time.time() - start_time, 60)
        
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            no_improve_epoch = 0
            torch.save(model.state_dict(), model_save_path)
            print(f"‚úÖ ƒê√£ l∆∞u model m·ªõi (Loss: {valid_loss:.3f})")
        else:
            no_improve_epoch += 1
            print(f"‚ö†Ô∏è Loss kh√¥ng gi·∫£m ({no_improve_epoch}/{patience})")
        
        print(f'Epoch: {epoch+1:02} | Time: {int(mins)}m {int(secs)}s')
        print(f'\tTrain Loss: {train_loss:.3f} | Val. Loss: {valid_loss:.3f}')
        
        if no_improve_epoch >= patience:
            print("üõë D·ª™NG S·ªöM (Early Stopping)!")
            break

    # --- 5. ƒê√°nh gi√° ---
    print("\n4. ƒê√ÅNH GI√Å K·∫æT QU·∫¢")
    if os.path.exists(model_save_path):
        model.load_state_dict(torch.load(model_save_path))
        print("ƒê√£ load l·∫°i best model ƒë·ªÉ ƒë√°nh gi√°.")
        
    score = calculate_bleu(val_dataset, train_dataset.src_vocab, train_dataset.trg_vocab, model, device)
    print(f'‚≠êÔ∏è BLEU Score = {score*100:.2f}')

    # --- 6. Inference Function (ƒê√öNG Y√äU C·∫¶U ·∫¢NH) ---
    def translate(sentence: str) -> str:
        """
        H√†m d·ªãch v·ªõi c√°c y√™u c·∫ßu:
        - Input: str
        - Output: str
        - Greedy Decoding
        - Max Length: 50
        - Stop at <eos>
        """
        # 1. Tokenize & X·ª≠ l√Ω Input
        if isinstance(sentence, str):
            tokens = tokenize_en(sentence)
        else:
            tokens = sentence 

        # Th√™m <sos>, <eos>
        tokens = ["<sos>"] + tokens + ["<eos>"]
        
        # Chuy·ªÉn th√†nh s·ªë
        src_indexes = train_dataset.src_vocab.numericalize(tokens)
        
        # Chuy·ªÉn th√†nh tensor
        src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)
        src_len = torch.LongTensor([len(src_indexes)])

        # 2. Encoder
        model.eval()
        with torch.no_grad():
            hidden, cell = model.encoder(src_tensor, src_len)

        # 3. Decoder Loop
        trg_indexes = [train_dataset.trg_vocab.stoi["<sos>"]]
        max_len = 50 
        
        for i in range(max_len):
            trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)
            
            with torch.no_grad():
                output, hidden, cell = model.decoder(trg_tensor, hidden, cell)
            
            # Greedy decoding: Ch·ªçn token x√°c su·∫•t cao nh·∫•t
            pred_token = output.argmax(1).item()
            
            # ƒêi·ªÅu ki·ªán d·ª´ng: G·∫∑p <eos>
            if pred_token == train_dataset.trg_vocab.stoi["<eos>"]:
                break
            
            trg_indexes.append(pred_token)

        # 4. Detokenize (S·ªë -> Ch·ªØ)
        trg_tokens = [train_dataset.trg_vocab.itos[i] for i in trg_indexes]
        
        # Lo·∫°i b·ªè <sos> ·ªü ƒë·∫ßu. (L∆∞u √Ω: <eos> ƒë√£ break, kh√¥ng ƒë∆∞·ª£c add v√†o list n√™n kh√¥ng c·∫ßn x√≥a ƒëu√¥i)
        result_tokens = trg_tokens[1:]
        
        return " ".join(result_tokens)

    # --- 7. Ch·∫ø ƒë·ªô t∆∞∆°ng t√°c ---
    print("\n-------------------------------------------")
    print("ü§ñ CH·∫æ ƒê·ªò D·ªäCH TH·ª¨ (G√µ 'q' ƒë·ªÉ tho√°t)")
    print("-------------------------------------------")
    while True:
        try:
            sentence = input("\nüá¨üáß English: ")
            if sentence.lower() in ['q', 'quit', 'exit']: 
                break
            
            # G·ªçi h√†m translate ƒë√∫ng chu·∫©n
            result = translate(sentence)
            
            print(f"üá´üá∑ French:  {result}")
        except Exception as e: 
            print(f"‚ùå L·ªói: {e}")