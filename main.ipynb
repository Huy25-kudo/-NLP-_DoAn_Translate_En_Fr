{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba236940",
   "metadata": {},
   "source": [
    "### Import th∆∞ vi·ªán v√† C·∫•u h√¨nh thi·∫øt b·ªã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d17a07b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîπ ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Import c√°c module t·ª± ƒë·ªãnh nghƒ©a (ƒë·∫£m b·∫£o file data_utils.py v√† model.py n·∫±m c√πng th∆∞ m·ª•c)\n",
    "from data_utils import Multi30kDataset, Collate, tokenize_en\n",
    "from model import Encoder, Decoder, Seq2Seq\n",
    "\n",
    "# C·∫•u h√¨nh thi·∫øt b·ªã\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üîπ ƒêang s·ª≠ d·ª•ng thi·∫øt b·ªã: {device}\")\n",
    "\n",
    "# C·∫•u h√¨nh ƒë∆∞·ªùng d·∫´n (L∆∞u √Ω: Notebook kh√¥ng c√≥ __file__, d√πng os.getcwd())\n",
    "current_dir = os.getcwd() \n",
    "data_dir = os.path.join(current_dir, 'data', 'multi30k_en_fr')\n",
    "\n",
    "# ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    print(f\"‚ö†Ô∏è Th∆∞ m·ª•c d·ªØ li·ªáu ch∆∞a t·ªìn t·∫°i: {data_dir}. H√£y ƒë·∫£m b·∫£o b·∫°n ƒë√£ t·∫£i d·ªØ li·ªáu.\")\n",
    "\n",
    "train_src = os.path.join(data_dir, 'train.en')\n",
    "train_trg = os.path.join(data_dir, 'train.fr')\n",
    "val_src = os.path.join(data_dir, 'val.en') \n",
    "val_trg = os.path.join(data_dir, 'val.fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e946f5",
   "metadata": {},
   "source": [
    "### Kh·ªüi t·∫°o Dataset v√† DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71a573ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. KH·ªûI T·∫†O D·ªÆ LI·ªÜU...\n",
      "üîπ ƒêang ƒë·ªçc: train.en...\n",
      "üîπ ƒêang ƒë·ªçc: val.en...\n",
      "‚úÖ ƒê√£ load d·ªØ li·ªáu. Vocab ngu·ªìn: 6191, Vocab ƒë√≠ch: 6555\n"
     ]
    }
   ],
   "source": [
    "print(\"1. KH·ªûI T·∫†O D·ªÆ LI·ªÜU...\")\n",
    "\n",
    "# Load dataset\n",
    "# L∆∞u √Ω: N·∫øu file ch∆∞a t·ªìn t·∫°i, Dataset c√≥ th·ªÉ b√°o l·ªói t√πy v√†o c√°ch b·∫°n vi·∫øt data_utils\n",
    "train_dataset = Multi30kDataset(train_src, train_trg)\n",
    "\n",
    "# D√πng chung vocab cho validation n·∫øu c√≥\n",
    "if os.path.exists(val_src):\n",
    "    val_dataset = Multi30kDataset(val_src, val_trg, src_vocab=train_dataset.src_vocab, trg_vocab=train_dataset.trg_vocab)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu Validation, d√πng t·∫°m t·∫≠p Train ƒë·ªÉ test code.\")\n",
    "    val_dataset = train_dataset\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "# T·∫°o DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                          collate_fn=Collate(train_dataset.src_vocab.stoi[\"<pad>\"]),\n",
    "                          num_workers=0, pin_memory=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, \n",
    "                        collate_fn=Collate(train_dataset.src_vocab.stoi[\"<pad>\"]),\n",
    "                        num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ load d·ªØ li·ªáu. Vocab ngu·ªìn: {len(train_dataset.src_vocab)}, Vocab ƒë√≠ch: {len(train_dataset.trg_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86617ee",
   "metadata": {},
   "source": [
    "### Kh·ªüi t·∫°o Model, Optimizer v√† Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91314077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. KH·ªûI T·∫†O MODEL...\n",
      "The model has 13,982,107 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "print(\"2. KH·ªûI T·∫†O MODEL...\")\n",
    "\n",
    "INPUT_DIM = len(train_dataset.src_vocab)\n",
    "OUTPUT_DIM = len(train_dataset.trg_vocab)\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "EPOCHS = 16\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "\n",
    "model = Seq2Seq(enc, dec, device).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "# ƒê·∫øm s·ªë l∆∞·ª£ng tham s·ªë\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=train_dataset.trg_vocab.stoi[\"<pad>\"])\n",
    "scaler = torch.amp.GradScaler('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7350d553",
   "metadata": {},
   "source": [
    "### ƒê·ªãnh nghƒ©a h√†m Train v√† Translate n·ªôi b·ªô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d5dd6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer, criterion, clip, scaler, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (src, trg, src_len) in enumerate(iterator):\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Mixed Precision Training\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            output = model(src, src_len, trg)\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            \n",
    "            loss = criterion(output, trg)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f\"   Batch {i}/{len(iterator)} | Loss: {loss.item():.4f}\")\n",
    "            \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def translate_sentence_internal(sentence, src_vocab, trg_vocab, model, device, max_len=50):\n",
    "    \"\"\"H√†m d·ªãch d√πng ƒë·ªÉ t√≠nh BLEU (tr·∫£ v·ªÅ list tokens)\"\"\"\n",
    "    model.eval()\n",
    "    if isinstance(sentence, str):\n",
    "        tokens = tokenize_en(sentence)\n",
    "    else:\n",
    "        tokens = sentence\n",
    "    tokens = [\"<sos>\"] + tokens + [\"<eos>\"]\n",
    "    src_indexes = src_vocab.numericalize(tokens)\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "    src_len = torch.LongTensor([len(src_indexes)])\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor, src_len)\n",
    "        \n",
    "    trg_indexes = [trg_vocab.stoi[\"<sos>\"]]\n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "        pred_token = output.argmax(1).item()\n",
    "        trg_indexes.append(pred_token)\n",
    "        if pred_token == trg_vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "            \n",
    "    trg_tokens = [trg_vocab.itos[i] for i in trg_indexes]\n",
    "    return trg_tokens[1:-1]\n",
    "\n",
    "def calculate_bleu(data, src_vocab, trg_vocab, model, device):\n",
    "    targets = []\n",
    "    outputs = []\n",
    "    print(\"   ƒêang t√≠nh BLEU tr√™n 200 m·∫´u ƒë·∫ßu ti√™n...\")\n",
    "    # Gi·ªõi h·∫°n 200 m·∫´u ƒë·ªÉ ti·∫øt ki·ªám th·ªùi gian\n",
    "    for i in range(min(len(data), 200)):\n",
    "        src = data.src_tokenized[i]\n",
    "        trg = data.trg_tokenized[i]\n",
    "        \n",
    "        pred = translate_sentence_internal(src, src_vocab, trg_vocab, model, device)\n",
    "        \n",
    "        targets.append([trg])\n",
    "        outputs.append(pred)\n",
    "        \n",
    "    return corpus_bleu(targets, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e567d35",
   "metadata": {},
   "source": [
    "### V√≤ng l·∫∑p Hu·∫•n luy·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c081d3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. ƒê√ÅNH GI√Å K·∫æT QU·∫¢\n",
      "‚úÖ ƒê√£ load model t·ªët nh·∫•t t·ª´: d:\\LTSM\\best-model.pth\n",
      "   ƒêang t√≠nh BLEU tr√™n 200 m·∫´u ƒë·∫ßu ti√™n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GIA HUY\\AppData\\Local\\Temp\\ipykernel_26236\\1461541616.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(save_path, map_location=device)) # Th√™m map_location ƒë·ªÉ tr√°nh l·ªói n·∫øu load tr√™n CPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÜ BLEU Score = 30.42\n"
     ]
    }
   ],
   "source": [
    "import os # Import l·∫°i cho ch·∫Øc ch·∫Øn n·∫øu ch·∫°y ƒë·ªôc l·∫≠p\n",
    "\n",
    "print(\"4. ƒê√ÅNH GI√Å K·∫æT QU·∫¢\")\n",
    "\n",
    "# ƒê·ªãnh nghƒ©a l·∫°i ƒë∆∞·ªùng d·∫´n file model (ƒë·ªÉ kh√¥ng ph·ª• thu·ªôc v√†o cell tr√™n)\n",
    "if 'current_dir' not in locals():\n",
    "    current_dir = os.getcwd()\n",
    "save_path = os.path.join(current_dir, 'best-model.pth')\n",
    "\n",
    "# Load l·∫°i model t·ªët nh·∫•t\n",
    "if os.path.exists(save_path):\n",
    "    model.load_state_dict(torch.load(save_path, map_location=device)) # Th√™m map_location ƒë·ªÉ tr√°nh l·ªói n·∫øu load tr√™n CPU\n",
    "    print(f\"‚úÖ ƒê√£ load model t·ªët nh·∫•t t·ª´: {save_path}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y file '{save_path}'. ƒêang s·ª≠ d·ª•ng model hi·ªán t·∫°i (ch∆∞a load l·∫°i).\")\n",
    "\n",
    "# T√≠nh to√°n BLEU\n",
    "score = calculate_bleu(val_dataset, train_dataset.src_vocab, train_dataset.trg_vocab, model, device)\n",
    "print(f'üèÜ BLEU Score = {score*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f66770b",
   "metadata": {},
   "source": [
    "### H√†m D·ªãch (Inference) v√† Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b54602c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- K·∫æT QU·∫¢ D·ªäCH TH·ª¨ ---\n",
      "üá¨üáß: Two young, white males are outside near many bushes.\n",
      "üá´üá∑: Deux jeunes hommes blancs sont dehors pr√®s de nombreux gens .\n",
      "------------------------------\n",
      "üá¨üáß: A man in an orange hat starring at something.\n",
      "üá´üá∑: Un homme avec un casque orange regardant quelque chose .\n",
      "------------------------------\n",
      "üá¨üáß: A group of people standing in front of an igloo.\n",
      "üá´üá∑: Un groupe de personnes debout devant une maison .\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "def translate(sentence, model, src_vocab, trg_vocab, device):\n",
    "    \"\"\"H√†m d·ªãch chu·ªói string ƒë·∫ßu v√†o sang ti·∫øng Ph√°p\"\"\"\n",
    "    \n",
    "    # 1. Tokenize\n",
    "    if isinstance(sentence, str):\n",
    "        tokens = tokenize_en(sentence)\n",
    "    else:\n",
    "        tokens = sentence \n",
    "\n",
    "    # Th√™m <sos>, <eos>\n",
    "    tokens = [\"<sos>\"] + tokens + [\"<eos>\"]\n",
    "    \n",
    "    # Chuy·ªÉn th√†nh ch·ªâ s·ªë \n",
    "    src_indexes = src_vocab.numericalize(tokens)\n",
    "    \n",
    "    # Chuy·ªÉn th√†nh tensor v√† th√™m chi·ªÅu batch\n",
    "    src_tensor = torch.LongTensor(src_indexes).unsqueeze(1).to(device)\n",
    "    src_len = torch.LongTensor([len(src_indexes)])\n",
    "\n",
    "    # 2. Encoder\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor, src_len)\n",
    "\n",
    "    # 3. Decoder\n",
    "    trg_indexes = [trg_vocab.stoi[\"<sos>\"]]\n",
    "    max_len = 50 \n",
    "    \n",
    "    for i in range(max_len):\n",
    "        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden, cell = model.decoder(trg_tensor, hidden, cell)\n",
    "        \n",
    "        # Greedy decoding\n",
    "        pred_token = output.argmax(1).item()\n",
    "        \n",
    "        if pred_token == trg_vocab.stoi[\"<eos>\"]:\n",
    "            break\n",
    "        \n",
    "        trg_indexes.append(pred_token)\n",
    "\n",
    "    # 4. Convert back to string\n",
    "    trg_tokens = [trg_vocab.itos[i] for i in trg_indexes]\n",
    "    \n",
    "    # B·ªè <sos> ·ªü ƒë·∫ßu\n",
    "    return \" \".join(trg_tokens[1:])\n",
    "\n",
    "# --- DEMO ---\n",
    "examples = [\n",
    "    \"Two young, white males are outside near many bushes.\",\n",
    "    \"A man in an orange hat starring at something.\",\n",
    "    \"A group of people standing in front of an igloo.\"\n",
    "]\n",
    "\n",
    "print(\"--- K·∫æT QU·∫¢ D·ªäCH TH·ª¨ ---\")\n",
    "for ex in examples:\n",
    "    trans = translate(ex, model, train_dataset.src_vocab, train_dataset.trg_vocab, device)\n",
    "    print(f\"üá¨üáß: {ex}\")\n",
    "    print(f\"üá´üá∑: {trans}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9118590",
   "metadata": {},
   "source": [
    "### Giao di·ªán D·ªãch t∆∞∆°ng t√°c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d698dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ch·∫°y cell n√†y ƒë·ªÉ t·ª± nh·∫≠p c√¢u\n",
    "while True:\n",
    "    try:\n",
    "        sentence = input(\"\\nNh·∫≠p c√¢u ti·∫øng Anh (g√µ 'q' ƒë·ªÉ tho√°t): \")\n",
    "        if sentence.lower() in ['q', 'quit']: \n",
    "            print(\"ƒê√£ tho√°t.\")\n",
    "            break\n",
    "        \n",
    "        result = translate(sentence, model, train_dataset.src_vocab, train_dataset.trg_vocab, device)\n",
    "        print(f\"üá´üá∑ French:  {result}\")\n",
    "    except Exception as e: \n",
    "        print(f\"L·ªói: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
